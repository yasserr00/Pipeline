services:
  postgres:
    image: postgres:15-alpine
    container_name: ml-pipeline-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
    - postgres_data:/var/lib/postgresql/data
    ports:
    - 5432:5432
    healthcheck:
      test:
      - CMD-SHELL
      - pg_isready -U ${POSTGRES_USER:-airflow}
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
    - ml-pipeline-network
    restart: unless-stopped
  mlflow:
    image: python:3.10-slim
    container_name: ml-pipeline-mlflow
    environment:
    - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
    - ./mlruns:/app/mlruns
    - ./models:/app/models
    - ./data:/app/data
    ports:
    - 5001:5000
    command: 'bash -c " apt-get update && apt-get install -y curl && pip install --no-cache-dir mlflow && mkdir -p /app/mlruns && chmod -R 777 /app/mlruns && mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri file:///app/mlruns --default-artifact-root /app/mlruns "

      '
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:5000/health
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
    - ml-pipeline-network
    restart: unless-stopped
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-pipeline-webserver
    command: webserver
    environment:
    - AIRFLOW_HOME=/app/airflow
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    - MLFLOW_TRACKING_URI=http://mlflow:5000
    - POSTGRES_USER=${POSTGRES_USER:-airflow}
    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
    - POSTGRES_DB=${POSTGRES_DB:-airflow}
    - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
    - POSTGRES_PORT=${POSTGRES_PORT:-5432}
    - MLFLOW_HOST=${MLFLOW_HOST:-mlflow}
    - MLFLOW_PORT=${MLFLOW_PORT:-5000}
    - AIRFLOW_ADMIN_USERNAME=${AIRFLOW_ADMIN_USERNAME:-admin}
    - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}
    - AIRFLOW_ADMIN_EMAIL=${AIRFLOW_ADMIN_EMAIL:-admin@example.com}
    - AIRFLOW_ADMIN_FIRSTNAME=${AIRFLOW_ADMIN_FIRSTNAME:-Admin}
    - AIRFLOW_ADMIN_LASTNAME=${AIRFLOW_ADMIN_LASTNAME:-User}
    volumes:
    - ./airflow/dags:/app/airflow/dags
    - ./airflow/logs:/app/airflow/logs
    - ./airflow/plugins:/app/airflow/plugins
    - ./data:/app/data
    - ./mlruns:/app/mlruns
    - ./models:/app/models
    - ./src:/app/src
    - ./mlflow_config.py:/app/mlflow_config.py
    - ./train_model.py:/app/train_model.py
    ports:
    - 8080:8080
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:${AIRFLOW_WEBSERVER_PORT:-8080}/health
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
    - ml-pipeline-network
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      mlflow:
        condition: service_healthy
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-pipeline-scheduler
    command: scheduler
    environment:
    - AIRFLOW_HOME=/app/airflow
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
    - MLFLOW_TRACKING_URI=http://mlflow:5000
    - POSTGRES_USER=${POSTGRES_USER:-airflow}
    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
    - POSTGRES_DB=${POSTGRES_DB:-airflow}
    - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
    - POSTGRES_PORT=${POSTGRES_PORT:-5432}
    - MLFLOW_HOST=${MLFLOW_HOST:-mlflow}
    - MLFLOW_PORT=${MLFLOW_PORT:-5000}
    volumes:
    - ./airflow/dags:/app/airflow/dags
    - ./airflow/logs:/app/airflow/logs
    - ./airflow/plugins:/app/airflow/plugins
    - ./data:/app/data
    - ./mlruns:/app/mlruns
    - ./models:/app/models
    - ./src:/app/src
    - ./mlflow_config.py:/app/mlflow_config.py
    - ./train_model.py:/app/train_model.py
    healthcheck:
      test:
      - CMD-SHELL
      - airflow jobs check --job-type SchedulerJob --hostname $$(hostname)
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
    - ml-pipeline-network
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-pipeline-api
    command: api
    environment:
    - MLFLOW_TRACKING_URI=http://mlflow:5000
    - API_EXPERIMENT=House_Price_Prediction
    - API_PORT=5050
    - MLFLOW_HOST=mlflow
    - MLFLOW_PORT=5000
    volumes:
    - ./data:/app/data
    - ./mlruns:/app/mlruns
    - ./models:/app/models
    - ./src:/app/src
    - ./mlflow_config.py:/app/mlflow_config.py
    - ./train_model.py:/app/train_model.py
    - ./main.py:/app/main.py
    - ./controller:/app/controller
    - ./front:/app/front
    ports:
    - 5050:5050
    networks:
    - ml-pipeline-network
    restart: unless-stopped
    depends_on:
      mlflow:
        condition: service_healthy
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:5050/health
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
volumes:
  postgres_data:
    driver: local
networks:
  ml-pipeline-network:
    driver: bridge
    name: ml-pipeline-network
